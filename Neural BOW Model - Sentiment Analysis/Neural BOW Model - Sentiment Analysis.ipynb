{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text data is already pretty clean, so not much preparation is required. Without getting too\n",
    "much into the details, we will prepare the data using the following method:\n",
    "Split tokens on white space.\n",
    "\n",
    "- Remove all punctuation from words.\n",
    "- Remove all words that are not purely comprised of alphabetical characters.\n",
    "- Remove all words that are known stop words.\n",
    "- Remove all words that have a length â‰¤ 1 character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vivek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44276\n",
      "25767\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # prepare regex for char filtering\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from each word\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    "\n",
    "# load doc and add to vocab\n",
    "def add_doc_to_vocab(filename, vocab):\n",
    "    # load doc\n",
    "    doc = load_doc(filename)\n",
    "    # clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "    # update counts\n",
    "    vocab.update(tokens)\n",
    "    \n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab):\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        if filename.startswith('cv9'):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # add doc to vocab\n",
    "        add_doc_to_vocab(path, vocab)\n",
    "        \n",
    "# save list to file\n",
    "def save_list(lines, filename):\n",
    "    # convert lines to a single blob of text\n",
    "    data = '\\n'.join(lines)\n",
    "    # open file\n",
    "    file = open(filename, 'w')\n",
    "    # write text\n",
    "    file.write(data)\n",
    "    # close file\n",
    "    file.close()\n",
    "        \n",
    "# define vocab\n",
    "vocab = Counter()\n",
    "# add all docs to vocab\n",
    "process_docs('txt_sentoken/pos', vocab)\n",
    "process_docs('txt_sentoken/neg', vocab)\n",
    "# print the size of the vocab\n",
    "print(len(vocab))\n",
    "# keep tokens with a min occurrence\n",
    "min_occurane = 2\n",
    "tokens = [k for k,c in vocab.items() if c >= min_occurane]\n",
    "print(len(tokens))\n",
    "# save tokens to a vocabulary file\n",
    "save_list(tokens, 'vocab.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to look at extracting features from the reviews ready for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800 1800\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # prepare regex for char filtering\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from each word\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    "\n",
    "# load doc, clean and return line of tokens\n",
    "def doc_to_line(filename, vocab):\n",
    "    # load the doc\n",
    "    doc = load_doc(filename)\n",
    "    # clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "    # filter by vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab):\n",
    "    lines = list()\n",
    "        # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        if filename.startswith('cv9'):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load and clean the doc\n",
    "        line = doc_to_line(path, vocab)\n",
    "        # add to list\n",
    "        lines.append(line)\n",
    "    return lines\n",
    "    \n",
    "# load and clean a dataset\n",
    "def load_clean_dataset(vocab):\n",
    "    # load documents\n",
    "    neg = process_docs('txt_sentoken/neg', vocab)\n",
    "    pos = process_docs('txt_sentoken/pos', vocab)\n",
    "    docs = neg + pos\n",
    "    # prepare labels\n",
    "    labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
    "    return docs, labels\n",
    "\n",
    "# load the vocabulary\n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    "# load all training reviews\n",
    "docs, labels = load_clean_dataset(vocab)\n",
    "# summarize what we have\n",
    "print(len(docs), len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800, 25768) (200, 25768)\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "#load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # prepare regex for char filtering\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from each word\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    "\n",
    "# load doc, clean and return line of tokens\n",
    "def doc_to_line(filename, vocab):\n",
    "    # load the doc\n",
    "    doc = load_doc(filename)\n",
    "    # clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "    # filter by vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab, is_train):\n",
    "    lines = list()\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        if is_train and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_train and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load and clean the doc\n",
    "        line = doc_to_line(path, vocab)\n",
    "        # add to list\n",
    "        lines.append(line)\n",
    "    return lines\n",
    "\n",
    "# load and clean a dataset\n",
    "def load_clean_dataset(vocab, is_train):\n",
    "    # load documents\n",
    "    neg = process_docs('txt_sentoken/neg', vocab, is_train)\n",
    "    pos = process_docs('txt_sentoken/pos', vocab, is_train)\n",
    "    docs = neg + pos\n",
    "    # prepare labels\n",
    "    labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
    "    return docs, labels\n",
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# load the vocabulary\n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = set(vocab.split())\n",
    "# load all reviews\n",
    "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
    "test_docs, ytest = load_clean_dataset(vocab, False)\n",
    "# create the tokenizer\n",
    "tokenizer = create_tokenizer(train_docs)\n",
    "# encode data\n",
    "Xtrain = tokenizer.texts_to_matrix(train_docs, mode='freq')\n",
    "Xtest = tokenizer.texts_to_matrix(test_docs, mode='freq')\n",
    "print(Xtrain.shape, Xtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "57/57 - 1s - loss: 0.6914 - accuracy: 0.5589\n",
      "Epoch 2/10\n",
      "57/57 - 1s - loss: 0.6806 - accuracy: 0.5922\n",
      "Epoch 3/10\n",
      "57/57 - 1s - loss: 0.6617 - accuracy: 0.7972\n",
      "Epoch 4/10\n",
      "57/57 - 1s - loss: 0.6331 - accuracy: 0.8722\n",
      "Epoch 5/10\n",
      "57/57 - 1s - loss: 0.5966 - accuracy: 0.9267\n",
      "Epoch 6/10\n",
      "57/57 - 1s - loss: 0.5541 - accuracy: 0.9417\n",
      "Epoch 7/10\n",
      "57/57 - 1s - loss: 0.5092 - accuracy: 0.9428\n",
      "Epoch 8/10\n",
      "57/57 - 1s - loss: 0.4639 - accuracy: 0.9578\n",
      "Epoch 9/10\n",
      "57/57 - 1s - loss: 0.4196 - accuracy: 0.9606\n",
      "Epoch 10/10\n",
      "57/57 - 1s - loss: 0.3792 - accuracy: 0.9656\n",
      "Test Accuracy: 87.000000\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "#load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # prepare regex for char filtering\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from each word\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    "\n",
    "# load doc, clean and return line of tokens\n",
    "def doc_to_line(filename, vocab):\n",
    "    # load the doc\n",
    "    doc = load_doc(filename)\n",
    "    # clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "    # filter by vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab, is_train):\n",
    "    lines = list()\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        if is_train and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_train and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load and clean the doc\n",
    "        line = doc_to_line(path, vocab)\n",
    "        # add to list\n",
    "        lines.append(line)\n",
    "    return lines\n",
    "\n",
    "# load and clean a dataset\n",
    "def load_clean_dataset(vocab, is_train):\n",
    "    # load documents\n",
    "    neg = process_docs('txt_sentoken/neg', vocab, is_train)\n",
    "    pos = process_docs('txt_sentoken/pos', vocab, is_train)\n",
    "    docs = neg + pos\n",
    "    # prepare labels\n",
    "    labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
    "    return docs, labels\n",
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# define the model\n",
    "def define_model(n_words):\n",
    "    # define network\n",
    "    model = Sequential()\n",
    "    model.add(Dense(50, input_shape=(n_words,), activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile network\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # summarize defined model\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "# load the vocabulary\n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = set(vocab.split())\n",
    "# load all reviews\n",
    "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
    "test_docs, ytest = load_clean_dataset(vocab, False)\n",
    "# create the tokenizer\n",
    "tokenizer = create_tokenizer(train_docs)\n",
    "# encode data\n",
    "Xtrain = tokenizer.texts_to_matrix(train_docs, mode='freq')\n",
    "Xtest = tokenizer.texts_to_matrix(test_docs, mode='freq')\n",
    "# define the model\n",
    "n_words = Xtest.shape[1]\n",
    "model = define_model(n_words)\n",
    "# fit network\n",
    "model.fit(Xtrain, np.array(ytrain), epochs=10, verbose=2)\n",
    "# evaluate\n",
    "loss, acc = model.evaluate(Xtest, np.array(ytest), verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_41\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_82 (Dense)             (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "1 accuracy: 0.925000011920929\n",
      "Model: \"sequential_42\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_84 (Dense)             (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_85 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2 accuracy: 0.925000011920929\n",
      "Model: \"sequential_43\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_86 (Dense)             (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_87 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "3 accuracy: 0.9399999976158142\n",
      "Model: \"sequential_44\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_88 (Dense)             (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_89 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4 accuracy: 0.9100000262260437\n",
      "Model: \"sequential_45\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_90 (Dense)             (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_91 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "5 accuracy: 0.925000011920929\n",
      "Model: \"sequential_46\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_92 (Dense)             (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_93 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "6 accuracy: 0.949999988079071\n",
      "Model: \"sequential_47\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_94 (Dense)             (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_95 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "7 accuracy: 0.9300000071525574\n",
      "Model: \"sequential_48\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_96 (Dense)             (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_97 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "8 accuracy: 0.925000011920929\n",
      "Model: \"sequential_49\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_98 (Dense)             (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_99 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "9 accuracy: 0.925000011920929\n",
      "Model: \"sequential_50\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_100 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_101 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "10 accuracy: 0.925000011920929\n",
      "Model: \"sequential_51\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_102 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_103 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "1 accuracy: 0.9049999713897705\n",
      "Model: \"sequential_52\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_104 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_105 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2 accuracy: 0.8999999761581421\n",
      "Model: \"sequential_53\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_106 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_107 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 accuracy: 0.8999999761581421\n",
      "Model: \"sequential_54\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_108 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_109 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4 accuracy: 0.9049999713897705\n",
      "Model: \"sequential_55\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_110 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_111 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "5 accuracy: 0.8949999809265137\n",
      "Model: \"sequential_56\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_112 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_113 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "6 accuracy: 0.8949999809265137\n",
      "Model: \"sequential_57\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_114 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_115 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "7 accuracy: 0.8999999761581421\n",
      "Model: \"sequential_58\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_116 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_117 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "8 accuracy: 0.9049999713897705\n",
      "Model: \"sequential_59\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_118 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_119 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "9 accuracy: 0.8999999761581421\n",
      "Model: \"sequential_60\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_120 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_121 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "10 accuracy: 0.9049999713897705\n",
      "Model: \"sequential_61\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_122 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_123 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "1 accuracy: 0.8899999856948853\n",
      "Model: \"sequential_62\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_124 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_125 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2 accuracy: 0.9150000214576721\n",
      "Model: \"sequential_63\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_126 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_127 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "3 accuracy: 0.8550000190734863\n",
      "Model: \"sequential_64\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_128 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_129 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4 accuracy: 0.8849999904632568\n",
      "Model: \"sequential_65\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_130 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_131 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "5 accuracy: 0.8700000047683716\n",
      "Model: \"sequential_66\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_132 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_133 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 accuracy: 0.8700000047683716\n",
      "Model: \"sequential_67\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_134 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_135 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "7 accuracy: 0.8949999809265137\n",
      "Model: \"sequential_68\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_136 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_137 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "8 accuracy: 0.8849999904632568\n",
      "Model: \"sequential_69\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_138 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_139 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "9 accuracy: 0.8849999904632568\n",
      "Model: \"sequential_70\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_140 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_141 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "10 accuracy: 0.8799999952316284\n",
      "Model: \"sequential_71\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_142 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_143 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "1 accuracy: 0.875\n",
      "Model: \"sequential_72\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_144 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_145 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2 accuracy: 0.8550000190734863\n",
      "Model: \"sequential_73\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_146 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_147 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "3 accuracy: 0.8650000095367432\n",
      "Model: \"sequential_74\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_148 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_149 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4 accuracy: 0.8799999952316284\n",
      "Model: \"sequential_75\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_150 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_151 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "5 accuracy: 0.875\n",
      "Model: \"sequential_76\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_152 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_153 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "6 accuracy: 0.8700000047683716\n",
      "Model: \"sequential_77\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_154 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_155 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "7 accuracy: 0.8700000047683716\n",
      "Model: \"sequential_78\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_156 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_157 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "8 accuracy: 0.8700000047683716\n",
      "Model: \"sequential_79\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_158 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_159 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 accuracy: 0.8600000143051147\n",
      "Model: \"sequential_80\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_160 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_161 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "10 accuracy: 0.8700000047683716\n",
      "          binary      count      tfidf       freq\n",
      "count  10.000000  10.000000  10.000000  10.000000\n",
      "mean    0.928000   0.901000   0.883000   0.869000\n",
      "std     0.010593   0.003944   0.016193   0.007379\n",
      "min     0.910000   0.895000   0.855000   0.855000\n",
      "25%     0.925000   0.900000   0.872500   0.866250\n",
      "50%     0.925000   0.900000   0.885000   0.870000\n",
      "75%     0.928750   0.905000   0.888750   0.873750\n",
      "max     0.950000   0.905000   0.915000   0.880000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from matplotlib import pyplot\n",
    "\n",
    "#load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # prepare regex for char filtering\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from each word\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    "\n",
    "# load doc, clean and return line of tokens\n",
    "def doc_to_line(filename, vocab):\n",
    "    # load the doc\n",
    "    doc = load_doc(filename)\n",
    "    # clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "    # filter by vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab, is_train):\n",
    "    lines = list()\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        if is_train and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_train and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load and clean the doc\n",
    "        line = doc_to_line(path, vocab)\n",
    "        # add to list\n",
    "        lines.append(line)\n",
    "    return lines\n",
    "\n",
    "# load and clean a dataset\n",
    "def load_clean_dataset(vocab, is_train):\n",
    "    # load documents\n",
    "    neg = process_docs('txt_sentoken/neg', vocab, is_train)\n",
    "    pos = process_docs('txt_sentoken/pos', vocab, is_train)\n",
    "    docs = neg + pos\n",
    "    # prepare labels\n",
    "    labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
    "    return docs, labels\n",
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# define the model\n",
    "def define_model(n_words):\n",
    "    # define network\n",
    "    model = Sequential()\n",
    "    model.add(Dense(50, input_shape=(n_words,), activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile network\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # summarize defined model\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "# evaluate a neural network model\n",
    "def evaluate_mode(Xtrain, ytrain, Xtest, ytest):\n",
    "    scores = list()\n",
    "    n_repeats = 10\n",
    "    n_words = Xtest.shape[1]\n",
    "    for i in range(n_repeats):\n",
    "        # define network\n",
    "        model = define_model(n_words)\n",
    "        # fit network\n",
    "        model.fit(Xtrain, ytrain, epochs=10, verbose=0)\n",
    "        # evaluate\n",
    "        _, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
    "        scores.append(acc)\n",
    "        print('%d accuracy: %s' % ((i+1), acc))\n",
    "    return scores\n",
    "\n",
    "# prepare bag of words encoding of docs\n",
    "def prepare_data(train_docs, test_docs, mode):\n",
    "    # create the tokenizer\n",
    "    tokenizer = Tokenizer()\n",
    "    # fit the tokenizer on the documents\n",
    "    tokenizer.fit_on_texts(train_docs)\n",
    "    # encode training data set\n",
    "    Xtrain = tokenizer.texts_to_matrix(train_docs, mode=mode)\n",
    "    # encode training data set\n",
    "    Xtest = tokenizer.texts_to_matrix(test_docs, mode=mode)\n",
    "    return Xtrain, Xtest\n",
    "\n",
    "# load the vocabulary\n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = set(vocab.split())\n",
    "# load all reviews\n",
    "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
    "test_docs, ytest = load_clean_dataset(vocab, False)\n",
    "# run experiment\n",
    "modes = ['binary', 'count', 'tfidf', 'freq']\n",
    "results = DataFrame()\n",
    "for mode in modes:\n",
    "    # prepare data for mode\n",
    "    Xtrain, Xtest = prepare_data(train_docs, test_docs, mode)\n",
    "    # evaluate model on data for mode\n",
    "    results[mode] = evaluate_mode(Xtrain, np.array(ytrain), Xtest, np.array(ytest))\n",
    "# summarize results\n",
    "print(results.describe())\n",
    "# plot results\n",
    "results.boxplot()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_81\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_162 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_163 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n",
      "Epoch 1/10\n",
      "63/63 - 1s - loss: 0.4427 - accuracy: 0.8120\n",
      "Epoch 2/10\n",
      "63/63 - 1s - loss: 0.0501 - accuracy: 0.9935\n",
      "Epoch 3/10\n",
      "63/63 - 1s - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "63/63 - 1s - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "63/63 - 1s - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "63/63 - 1s - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "63/63 - 1s - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "63/63 - 1s - loss: 7.0482e-04 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "63/63 - 1s - loss: 5.0934e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "63/63 - 1s - loss: 3.8719e-04 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f7d0f23288>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # prepare regex for char filtering\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from each word\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    "\n",
    "# load doc, clean and return line of tokens\n",
    "def doc_to_line(filename, vocab):\n",
    "    # load the doc\n",
    "    doc = load_doc(filename)\n",
    "    # clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "    # filter by vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab):\n",
    "    lines = list()\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load and clean the doc\n",
    "        line = doc_to_line(path, vocab)\n",
    "        # add to list\n",
    "        lines.append(line)\n",
    "    return lines\n",
    "\n",
    "# load and clean a dataset\n",
    "def load_clean_dataset(vocab):\n",
    "    # load documents\n",
    "    neg = process_docs('txt_sentoken/neg', vocab)\n",
    "    pos = process_docs('txt_sentoken/pos', vocab)\n",
    "    docs = neg + pos\n",
    "    # prepare labels\n",
    "    labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
    "    return docs, labels\n",
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# define the model\n",
    "def define_model(n_words):\n",
    "    # define network\n",
    "    model = Sequential()\n",
    "    model.add(Dense(50, input_shape=(n_words,), activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile network\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # summarize defined model\n",
    "    model.summary()\n",
    "    plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model\n",
    "\n",
    "# classify a review as negative or positive\n",
    "def predict_sentiment(review, vocab, tokenizer, model):\n",
    "    # clean\n",
    "    tokens = clean_doc(review)\n",
    "    # filter by vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    # convert to line\n",
    "    line = ' '.join(tokens)\n",
    "    # encode\n",
    "    encoded = tokenizer.texts_to_matrix([line], mode='binary')\n",
    "    # predict sentiment\n",
    "    yhat = model.predict(encoded, verbose=0)\n",
    "    # retrieve predicted percentage and label\n",
    "    percent_pos = yhat[0,0]\n",
    "    if round(percent_pos) == 0:\n",
    "        return (1-percent_pos), 'NEGATIVE'\n",
    "    return percent_pos, 'POSITIVE'\n",
    "\n",
    "# load the vocabulary\n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = set(vocab.split())\n",
    "# load all reviews\n",
    "train_docs, ytrain = load_clean_dataset(vocab)\n",
    "test_docs, ytest = load_clean_dataset(vocab)\n",
    "# create the tokenizer\n",
    "tokenizer = create_tokenizer(train_docs)\n",
    "# encode data\n",
    "Xtrain = tokenizer.texts_to_matrix(train_docs, mode='binary')\n",
    "Xtest = tokenizer.texts_to_matrix(test_docs, mode='binary')\n",
    "# define network\n",
    "n_words = Xtrain.shape[1]\n",
    "model = define_model(n_words)\n",
    "# fit network\n",
    "model.fit(Xtrain, np.array(ytrain), epochs=10, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: [Best movie ever! It was great.]\n",
      "Sentiment: POSITIVE (54.522%)\n",
      "Review: [This is a bad movie.]\n",
      "Sentiment: NEGATIVE (65.000%)\n"
     ]
    }
   ],
   "source": [
    "# test positive text\n",
    "text = 'Best movie ever! It was great.'\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))\n",
    "# test negative text\n",
    "text = 'This is a bad movie.'\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
